from collections import defaultdict, Counter, OrderedDict
import math
import itertools
from nltk import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from WordCloud import TagCloud

'''
    Please note that we decided to use modules instead of classes,
    because there is not really any shared state to justify the use of a class.
    This is also the same approach adopted in CIS 530.

'''

'''
    Important Helper Function - Tokenizer
'''


def tokenize(rawexcerpt):
    """
    :param rawexcerpt: string input (e.g. paragraph that contains many words)
    :return: list of tokenized words, all lower-cased
    """
    decoded_list = word_tokenize(rawexcerpt.decode('utf8'))
    return [word.encode('utf8').lower() for word in decoded_list]


'''
    Scikit Learn Approach
'''


def build_vector(corpus, stop_list):
    """
    We use sklearn here to build our vectors. Please especially note that we use our own nltk tokenizer,
    no smoothing, and l2 or dot product normalization. More importantly we use the stop_list we built.

    :param corpus: list of string reviews. E.g. ["this tastes good", "bad restaurant"]
    :param stop_list: list of stop words. We will build out the stop words ourselves.
    """
    results = defaultdict(lambda: 0)  # mapping of token and its corresponding tf-idf score

    # first build vectors using sklearn
    sklearn_tfidf = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True,
                                    tokenizer=tokenize, stop_words=stop_list)
    sklearn_representation = sklearn_tfidf.fit_transform(corpus)

    # now get the list of unique tokens in corpus
    feature_names = sklearn_tfidf.get_feature_names()

    # now fill the results dictionary {token: tf-idf}
    for col in sklearn_representation.nonzero()[1]:
        results[feature_names[col]] = sklearn_representation[0, col]

    return results


def get_stopwords(census_last, census_first_f, census_first_m):
    """
    This function builds the stop list using the common list in CIS 530
    along with the wn.corpus stop list and the census (chose top 2500 first and last names)

    :param census_last: address of the census file for last names
    :param census_first_f: address of the census file for female first names
    :param census_first_m: address of the census file for male first names
    :return: set of stop words. Maintain set structure for better "not in" speed
    """
    out_list = []
    # add stopwords from common stop word list
    with open("stopwords.txt", "r") as fin:
        for line in fin:
            out_list.append(line.rstrip())
    # add the nltk stop words
    out_list.extend(stopwords.words('english'))

    # add some restaurant specific stop words
    out_list.append("restaurant")
    out_list.append("place")

    # Note that we are adding the top 2500 first and last names (arbitrary size of 2500)
    # 1. add the top 1250 common last names
    out_list.extend(common_names(census_last)[:1250])

    # 2. add the top common male and female first names, 625 each
    out_list.extend(common_names(census_first_f)[:625])
    out_list.extend(common_names(census_first_m)[:625])

    return set(out_list)


def common_names(file_address):
    """
    Get the common names from the census file. Each name string is lower cased.
    :param file_address: Census txt file detailing the distribution of common names
    :return: The full list of names in the census file
    """
    out_list = []
    with open(file_address, "r") as fin:
        for line in fin:
            out_list.append(line.split()[0].lower())
    return out_list


'''
    Home-brew tf-idf
'''


def flatten(listoflists):
    """
    Flatten a 2d list into an 1-d list
    :param listoflists: list of list of tokens
    :return: list of tokens
    """
    return list(itertools.chain(*listoflists))


def get_tf(sample):
    """
    Calculates the term frequency for words in the input sample
    :param sample: a list of tokens, generated by tokenize
    :return: : a dict with a key for each unique word in sample and
            int values giving the term frequency for each key
    """
    return Counter(sample)


def get_idf(corpus):
    """
    Get the inverse document frequency for each unique word in the input corpus.
    :param corpus: list of document lists, which contains list of lower-cased tokens
    :return: dict with a key for each unique word in the input corpus
            and float values giving the IDF for each key
    """
    idf_dict = {}
    df_dict = defaultdict(int)
    num_of_excerpts = len(corpus)

    # First get the df for unique words in the corpus
    keys = set(flatten(corpus))
    for words in keys:
        for excerpts in corpus:
            if words in excerpts:
                df_dict[words] += 1
    for items in df_dict:
        idf_dict[items] = math.log(float(num_of_excerpts) / df_dict[items])
    return idf_dict


'''
    Build Word Cloud
'''


def build_word_cloud(token_tf_idf_dict, num_of_words_to_display, picture_addr):
    # initiate the TagCloud object
    """
    Get a mapping of tokens and tf idf, generate the word cloud and save to given address.
    :param token_tf_idf_dict: dict of the mapping of tokens and tf idf, can be generated by build_vector
    :param num_of_words_to_display: int cap of the number of words to display
    :param picture_addr: string specifying to address to save our word cloud
    :return:
    """
    cloud = TagCloud()

    # we will then build up the data in required format to draw
    drawing_data = []

    # get the top n tokens with the highest tf idf
    num_of_words_to_display = min(len(token_tf_idf_dict.keys()), num_of_words_to_display)
    top_n_tokens = sorted(token_tf_idf_dict, key=lambda x: (-token_tf_idf_dict[x], x))[:num_of_words_to_display]

    for token in top_n_tokens:
        # this is the require format for drawing word cloud
        text_weight_mapping = {"text": token, "weight": token_tf_idf_dict[token]}
        drawing_data.append(text_weight_mapping)

    return cloud.draw(drawing_data, picture_addr)


def run(corpus, num_of_words_to_display=50, picture_addr="word_cloud.jpg", census_last_name="census-dist-2500-last.txt",
        census_first_female="census-dist-female-first.txt", census_first_male="census-dist-male-first.txt"):
    """
    This is the function that puts everything together. Input the corpus and let is run.
    :param corpus: list of string reviews. E.g. ["this tastes good", "bad restaurant"]
    :param num_of_words_to_display: default to 50 words to display
    :param picture_addr: default to "word_cloud.jpg" as output file name
    :param census_last_name: default to assume the census file is in current folder
    :param census_first_female: same as above
    :param census_first_male: same as above
    """
    stop_list = get_stopwords(census_last_name, census_first_female, census_first_male)
    tf_idf_dict = build_vector(corpus, stop_list)
    build_word_cloud(tf_idf_dict, num_of_words_to_display, picture_addr)


if __name__ == "__main__":
    # census_last_name = "census-dist-2500-last.txt"
    # census_first_female = "census-dist-female-first.txt"
    # census_first_male = "census-dist-male-first.txt"
    # run(["This restaurant is great", "This restaurant is authentic!", "This Chinese restaurant is Tasty.", "This place is authentic!", "This place is tasty."], 50, "test.jpg")
    print "done"
